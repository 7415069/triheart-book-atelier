# /app/services.py
import asyncio
import json
import os
from typing import List, Tuple, Callable, Dict, Any, Sequence, Generic

import httpx
from brtech_backend.core import database
# å¼•å…¥åç«¯æ ¸å¿ƒä¾èµ–
from brtech_backend.core.config import app_settings
from brtech_backend.core.models import M
from brtech_backend.core.schemas import UniqueConstraint
from brtech_backend.core.services import StringPKeyRecurseService, StringPKeyService
from brtech_backend.dictionary.services import StringPKeyWithDictionaryService
from brtech_backend.payment.handler import PaymentDispatcher, PaymentServiceMixin
from brtech_backend.payment.models import PayOrderModel
from brtech_backend.task.services import task_manager
from fastapi import HTTPException
from fastapi.concurrency import run_in_threadpool

from .ai_helper import AiHelper
# å¼•å…¥æœ¬é¡¹ç›®ä¾èµ–
from .crud import TriHeartPageCrud, TriHeartBookCrud, TriHeartChapterCrud, TriHeartChapterPageCrud, TriHeartBookNoteCrud, TriHeartBookUserCrud, TriHeartTermCrud, TriHeartPageTermCrud, TriHeartPageAttachmentCrud
from .models import TriHeartPageModel, TriHeartBookModel, TriHeartChapterModel, TriHeartChapterPageModel, TriHeartBookUserModel, TriHeartBookNoteModel, TriHeartPageTermModel, TriHeartTermModel, TriHeartPageAttachmentModel
from .pdf_helper import PdfStructure, PdfPage, PdfHelper
from .schemas import TriHeartPageQuery, TriHeartBookQuery, TriHeartChapterQuery, TriHeartChapterPageQuery, TriHeartBookUserQuery, TriHeartBookNoteQuery, TriHeartTermQuery, TriHeartPageTermQuery, TriHeartPageAttachmentQuery

# =========================================================
# é…ç½®éƒ¨åˆ†
# =========================================================
# class StorageConfig:
#   ROOT_DIR = "uploads/webp"
#   DB_PREFIX = "upload/books"


# çŠ¶æ€å¸¸é‡
PROC_PENDING = "0"
PROC_PROCESSING = "1"
PROC_SUCCESS = "2"
PROC_FAILED = "9"


# =========================================================
# 1. å¤–éƒ¨ Wrapper
# =========================================================
async def run_pdf_task_wrapper(user_id: str, book_id: str, task_id: str):
  """
  åå°ä»»åŠ¡å…¥å£ã€‚
  è´Ÿè´£ç®¡ç† DB Session çš„ç”Ÿå‘½å‘¨æœŸï¼Œå¹¶å¯åŠ¨ Serviceã€‚
  """
  session_maker = database.get_session_maker()
  async with session_maker() as new_db:
    service = TriHeartBookService(new_db)
    await service.process_pdf_logic(user_id, book_id, task_id)


async def run_extract_terms_task(user_id: str, book_id: str, from_page: int, to_page: int, task_id: str):
  """AI æå–ä»»åŠ¡ Wrapper"""
  session_maker = database.get_session_maker()
  async with session_maker() as new_db:
    service = TriHeartBookService(new_db)
    await service.ai_extraction_logic(user_id, book_id, from_page, to_page)


async def run_scan_coords_task(user_id: str, book_id: str, task_id: str):
  """åæ ‡æ‰«æä»»åŠ¡ Wrapper"""
  session_maker = database.get_session_maker()
  async with session_maker() as new_db:
    service = TriHeartBookService(new_db)
    await service.scan_coordinates_logic(user_id, book_id)


# =========================================================
# 2. PDF è§£æå·¥å…·ç±» (CPU å¯†é›†å‹é€»è¾‘)
# =========================================================
class PdfProcessor:

  @staticmethod
  def _ensure_dir(path: str):
    if not os.path.exists(path):
      os.makedirs(path)

  @staticmethod
  def execute(pdf_path: str, webp_home: str, progress_callback: Callable[[int, int, str], None] = None) -> Tuple[bool, str, List[PdfPage], List[PdfStructure]]:
    """
    å¼€å§‹è§£æã€‚
    è¯·ç¡®ä¿åœ¨è°ƒç”¨å‰å·²è®¾ç½® self.webp_home, self.padding ç­‰å±æ€§ã€‚
    """
    # ç‰©ç†è·¯å¾„ç”¨äºä¿å­˜æ–‡ä»¶

    # ç§»é™¤å¼€å¤´çš„ / ä»¥é€‚é…ç›¸å¯¹è·¯å¾„
    pdf_path: str = pdf_path.lstrip("/")
    webp_home = webp_home.lstrip("/")
    PdfProcessor._ensure_dir(webp_home)

    pdf_helper = PdfHelper(pdf_path=pdf_path, webp_home=webp_home, progress_callback=progress_callback)
    return pdf_helper.parse()


# =========================================================
# 3. å„ä¸šåŠ¡ Service
# =========================================================

class TriHeartChapterService(StringPKeyRecurseService[TriHeartChapterModel, TriHeartChapterCrud, TriHeartChapterQuery]):

  async def remove_by_book_id(self, user_id: str, book_id: str, commit: bool = True) -> int:
    """æ ¹æ® BookID æ¸…ç†æ—§ç« èŠ‚"""
    rtn_val: int = 0
    try:
      rtn_val = await self.get_crud().remove_by_book_id(book_id)
      if commit:
        await self.get_crud().commit()
    except Exception as e:
      if commit:
        await self.get_crud().rollback()
        raise e
    return rtn_val


class TriHeartPageService(StringPKeyService[TriHeartPageModel, TriHeartPageCrud, TriHeartPageQuery]):

  def get_crud(self) -> TriHeartPageCrud:
    return super().get_crud()

  async def remove_by_book_id(self, user_id: str, book_id: str, commit: bool = True) -> int:
    rtn_val: int = 0
    try:
      rtn_val = await self.get_crud().remove_by_book_id(book_id)
      if commit:
        await self.get_crud().commit()
    except Exception as e:
      if commit:
        await self.get_crud().rollback()
        raise e
    return rtn_val

  async def remove_by_chapter_id(self, user_id: str, chapter_id: str, commit: bool = True) -> int:
    rtn_val: int = 0
    try:
      rtn_val = await self.get_crud().remove_by_chapter_id(chapter_id)
      if commit:
        await self.get_crud().commit()
    except Exception as e:
      if commit:
        await self.get_crud().rollback()
        raise e
    return rtn_val

  async def get_webp_url(self, user_id: str | None, book_id: str, page_no: int, webp_type: str) -> str:
    """æ ¹æ® BookID å’Œ PageNo è·å– Webp URL"""
    # 1. æ•°æ®åº“å•æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰é‰´æƒä¿¡æ¯
    row = await self.get_crud().custom_query_book_user_page(book_id, user_id, page_no)

    if not row:
      raise HTTPException(status_code=404, detail="è¯·æ±‚çš„é¡µé¢èµ„æºä¸å­˜åœ¨")

    # 2. é‰´æƒé€»è¾‘
    is_allowed = False

    # row å­—æ®µ: 0:path, 1:crop_path, 2:guest_limit, 3:user_limit, 4:purchase_status

    # ä¼˜å…ˆçº§ 1: å·²è´­ä¹° (purchase_status == '1') -> å…è®¸
    if row.purchase_status == '1':
      is_allowed = True

    # ä¼˜å…ˆçº§ 2: å·²ç™»å½• (user_id å­˜åœ¨) -> ä½¿ç”¨ user_preview_limit
    elif user_id:
      if page_no <= row.user_preview_limit:
        is_allowed = True

    # ä¼˜å…ˆçº§ 3: åŒ¿å (user_id ä¸ºç©º) -> ä½¿ç”¨ guest_preview_limit
    else:
      if page_no <= row.guest_preview_limit:
        is_allowed = True

    if not is_allowed:
      raise HTTPException(status_code=403, detail="è¶…å‡ºè¯•è¯»èŒƒå›´ï¼Œè¯·è´­ä¹°åç»§ç»­é˜…è¯»")

    # 3. èµ„æºè·¯å¾„å¤„ç†
    target_path = row.page_image_crop_path if webp_type == 'crop' else row.page_image_path

    # å®¹é”™ï¼šå¦‚æœ crop è·¯å¾„ä¸ºç©ºï¼Œå›é€€åˆ°åŸå›¾
    if webp_type == 'crop' and not target_path:
      target_path = row.page_image_path

    if not target_path:
      raise HTTPException(status_code=404, detail="å›¾ç‰‡èµ„æºç¼ºå¤±")

    return await self.get_oss_download_sign_url(user_id, target_path, "")


# [æ–°å¢] ç« èŠ‚ä¸ä¹¦é¡µå…³è”è¡¨ Service
class TriHeartChapterPageService(StringPKeyService[TriHeartChapterPageModel, TriHeartChapterPageCrud, TriHeartChapterPageQuery]):
  pass


class TriHeartBookService(StringPKeyWithDictionaryService[TriHeartBookModel, TriHeartBookCrud, TriHeartBookQuery]):

  async def pre_create(self, user_id: str, model: TriHeartBookModel) -> None:
    await super().pre_create(user_id, model)
    model.owner_id = user_id
    model.process_status = PROC_PENDING
    if not model.book_status:
      model.book_status = "3"

  # async def post_create(self, user_id: str, model: TriHeartBookModel) -> None:
  #   await super().post_create(user_id, model)
  #   if model.book_pdf_path:
  #     # è°ƒåº¦å¤–éƒ¨ Wrapper
  #     asyncio.create_task(run_pdf_task_wrapper(user_id, model.model_id))

  async def pre_update(self, user_id: str, old_model: TriHeartBookModel, new_model: TriHeartBookModel, update_fields: set[str]) -> None:
    await super().pre_update(user_id, old_model, new_model, update_fields)
    if "book_pdf_path" in update_fields and not new_model.book_pdf_path:
      new_model.book_pdf_path = old_model.book_pdf_path
    if "book_pdf_path" in update_fields and new_model.book_pdf_path and new_model.book_pdf_path != old_model.book_pdf_path:
      old_model.process_status = PROC_PENDING

  # async def post_update(self, user_id: str, old_model: TriHeartBookModel, new_model: TriHeartBookModel, update_fields: set[str]) -> None:
  #   await super().post_update(user_id, old_model, new_model, update_fields)
  #   # ä»…å½“ PDF è·¯å¾„å˜æ›´æ—¶è§¦å‘
  #   if "book_pdf_path" in update_fields and new_model.book_pdf_path and new_model.book_pdf_path != old_model.book_pdf_path:
  #     asyncio.create_task(run_pdf_task_wrapper(user_id, old_model.model_id))

  async def process_pdf_logic(self, user_id: str, book_id: str, task_id: str):
    chapter_service = TriHeartChapterService(self.db)
    page_service = TriHeartPageService(self.db)
    # [æ–°å¢] å…³è”è¡¨æœåŠ¡
    cp_service = TriHeartChapterPageService(self.db)

    # 1. è·å–ä¹¦ç±
    book = await self.get(user_id, book_id)
    if not book or not book.book_pdf_path:
      self.logger.warning(f"ä¹¦ç± {book_id} ä¸å­˜åœ¨æˆ–è€…æ²¡æœ‰ PDF è·¯å¾„")
      return

    try:
      # 2. æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
      book.process_status = PROC_PROCESSING
      await self.update(user_id, book, commit=True)  # ç«‹å³æäº¤çŠ¶æ€æ›´æ–°

      self.logger.info(f"Task Start: {book.book_title} ({book_id})")

      loop = asyncio.get_event_loop()

      def _progress_notify_handler(current, total, msg):
        if current % 10 == 0 or current == total:
          percent = int(current / total * 100)
          self.logger.info(f"[{percent}%] {msg}")
          if task_id:
            asyncio.run_coroutine_threadsafe(task_manager.update_progress(task_id, percent, msg), loop)

      var_prefix = "var/"
      pdf_path = f"{var_prefix}{book.book_pdf_path}"

      if not os.path.exists(pdf_path):
        os.makedirs(os.path.dirname(pdf_path), exist_ok=True)
        pdf_signed_url = await self.get_oss_download_sign_url(user_id, book.book_pdf_path)

        self.logger.info(f"Downloading PDF from OSS to {pdf_path}...")

        try:
          async with httpx.AsyncClient() as client:
            # ä½¿ç”¨ stream æ¨¡å¼ï¼Œé¿å…å¤§æ–‡ä»¶çˆ†å†…å­˜
            async with client.stream("GET", pdf_signed_url) as response:
              response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 2xx åˆ™æŠ›å‡ºå¼‚å¸¸

              # ä»¥äºŒè¿›åˆ¶å†™æ¨¡å¼æ‰“å¼€æœ¬åœ°æ–‡ä»¶
              with open(pdf_path, "wb") as f:
                # å¼‚æ­¥è¿­ä»£æ•°æ®å—å†™å…¥
                async for chunk in response.aiter_bytes(chunk_size=8192):
                  f.write(chunk)

          self.logger.info("PDF Download successful.")
        except Exception as download_err:
          self.logger.error(f"Download failed: {download_err}")
          # ä¸‹è½½å¤±è´¥æ—¶ï¼Œå»ºè®®åˆ é™¤å¯èƒ½æŸåçš„åŠæˆå“æ–‡ä»¶
          if os.path.exists(pdf_path):
            os.remove(pdf_path)
          raise download_err

      webp_home = f"{os.path.dirname(pdf_path)}/webp"

      # 3. æ‰§è¡Œ CPU å¯†é›†å‹è§£æ
      success, msg, pages_data, chapters_data = await run_in_threadpool(PdfProcessor.execute, pdf_path=pdf_path, webp_home=webp_home, progress_callback=_progress_notify_handler)

      if success:
        # =======================================================
        # å¼€å¯æ•°æ®åº“äº‹åŠ¡æµç¨‹ (åˆ©ç”¨ commit=False)
        # =======================================================

        # 4.1 æ¸…ç†æ—§æ•°æ® (Commit=False)
        await chapter_service.remove_by_book_id(user_id, book_id, commit=False)
        await page_service.remove_by_book_id(user_id, book_id, commit=False)

        # 4.2 å¤„ç†ç« èŠ‚ (Chapter)
        _flat_triheart_chapter_models: list[TriHeartChapterModel] = []

        def _convert_to_triheart_chapter(pdf_structure: PdfStructure) -> TriHeartChapterModel:
          # åˆ›å»ºå½“å‰ç« èŠ‚å¯¹è±¡
          rtn_val: TriHeartChapterModel = TriHeartChapterModel(
              book_id=book_id,
              chapter_title=pdf_structure.title,
              from_page_no=pdf_structure.begin_page_no,
              to_page_no=pdf_structure.end_page_no
          )
          # æ”¶é›†åˆ°æ‰å¹³åˆ—è¡¨ï¼Œä¾›åç»­ç”Ÿæˆå…³è”å…³ç³»ä½¿ç”¨
          _flat_triheart_chapter_models.append(rtn_val)

          # é€’å½’å¤„ç†å­ç« èŠ‚
          _children: list[PdfStructure] = pdf_structure.children
          if _children:
            for child in _children:
              _child: TriHeartChapterModel = _convert_to_triheart_chapter(child)
              if rtn_val.children is None:
                rtn_val.children = []
              rtn_val.children.append(_child)
          return rtn_val

        triheart_chapter_models: list[TriHeartChapterModel] = []
        for chapter_data in chapters_data:
          triheart_chapter_model: TriHeartChapterModel = _convert_to_triheart_chapter(chapter_data)
          triheart_chapter_models.append(triheart_chapter_model)

        # æ‰¹é‡å…¥åº“ç« èŠ‚ (commit=False)
        await chapter_service.create_batch(user_id, triheart_chapter_models, commit=False)

        # 4.3 æ‰¹é‡å…¥åº“ä¹¦é¡µ (Pages) & ä¸Šä¼ å›¾ç‰‡åˆ° OSS
        triheart_page_models: list[TriHeartPageModel] = []

        # åˆå§‹åŒ– HTTP Client ç”¨äºä¸Šä¼ ï¼Œé¿å…å¾ªç¯å†…é‡å¤åˆ›å»ºè¿æ¥
        async with httpx.AsyncClient() as oss_client:

          # å®šä¹‰å†…éƒ¨ä¸Šä¼ å‡½æ•°
          async def _upload_image(local_path: str, object_key: str):
            if not os.path.exists(local_path):
              return
            try:
              content_type: str = "image/webp"
              upload_sign_url: dict[str, Any] = await self.get_oss_upload_sign_url(user_id, object_key, content_type=content_type)
              # è¯»å–æ–‡ä»¶å¹¶ä¸Šä¼  (WebP å›¾ç‰‡ä¸€èˆ¬è¾ƒå°ï¼Œç›´æ¥ read() é—®é¢˜ä¸å¤§ï¼Œä¹Ÿå¯ç”¨ stream)
              with open(local_path, "rb") as f:
                file_content = f.read()

              _response = await oss_client.put(
                  url=upload_sign_url["uploadUrl"],
                  content=file_content,
                  headers=upload_sign_url["headers"]
              )
              _response.raise_for_status()
            except Exception as upload_e:
              self.logger.error(f"Failed to upload {object_key}: {upload_e}")
              raise upload_e

          total_pages = len(pages_data)
          for idx, page_data in enumerate(pages_data):
            cropped_webp_path = page_data.cropped_webp_path
            original_webp_path = page_data.original_webp_path

            # è®¡ç®— Object Key (å»é™¤æœ¬åœ° var/ å‰ç¼€)
            # ä½¿ç”¨ removeprefix æ˜¯ Python 3.9+ çš„å®‰å…¨å†™æ³•
            object_key_crop = cropped_webp_path.removeprefix(var_prefix)
            object_key_orig = original_webp_path.removeprefix(var_prefix)

            triheart_page_model: TriHeartPageModel = TriHeartPageModel(
                book_id=book_id,
                page_no=page_data.page_no,
                page_content="\n".join(page_data.content) if page_data.content else "",
                crop_box_data=page_data.crop_box_data
            )
            triheart_page_model.page_image_crop_path = object_key_crop
            triheart_page_model.page_image_path = object_key_orig
            triheart_page_models.append(triheart_page_model)

            # å¹¶å‘æ‰§è¡Œå½“å‰é¡µçš„è£å‰ªå›¾å’ŒåŸå›¾ä¸Šä¼ 
            # ä½¿ç”¨ gather å¯ä»¥åŒæ—¶å‘èµ·ä¸¤ä¸ªä¸Šä¼ è¯·æ±‚
            await asyncio.gather(
                _upload_image(cropped_webp_path, object_key_crop),
                _upload_image(original_webp_path, object_key_orig)
            )

            if (idx + 1) % 10 == 0:
              self.logger.info(f"Uploading images: {idx + 1}/{total_pages}")

        await page_service.create_batch(user_id, triheart_page_models, commit=False)

        # 4.4 å»ºç«‹ ç« èŠ‚-ä¹¦é¡µ å…³è” (Relation) - [ä¼˜åŒ–ç‰ˆ]
        # å»ºç«‹ é¡µç  -> PageID çš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸
        page_map: Dict[int, str] = {
          p.page_no: p.model_id
          for p in triheart_page_models
          if p.page_no is not None
        }

        triheart_chapter_page_models: list[TriHeartChapterPageModel] = []

        # éå†æ‰€æœ‰ç« èŠ‚ï¼Œæ ¹æ®å…¶èŒƒå›´ç›´æ¥æŸ¥æ‰¾é¡µç 
        for _flat_chapter_model in _flat_triheart_chapter_models:
          start_page = _flat_chapter_model.from_page_no
          end_page = _flat_chapter_model.to_page_no

          if start_page is not None and end_page is not None:
            # ç›´æ¥ç”Ÿæˆè¯¥ç« èŠ‚è¦†ç›–çš„é¡µç èŒƒå›´
            for p_no in range(start_page, end_page + 1):
              # O(1) æŸ¥æ‰¾ PageID
              if p_no in page_map:
                rel = TriHeartChapterPageModel(
                    chapter_id=_flat_chapter_model.model_id,
                    page_id=page_map[p_no]
                )
                triheart_chapter_page_models.append(rel)

        if triheart_chapter_page_models:
          await cp_service.create_batch(user_id, triheart_chapter_page_models, commit=False)

        # 4.5 æ›´æ–°ä¹¦ç±ä¿¡æ¯
        # å¦‚æœæ²¡æœ‰å°é¢ï¼Œä½¿ç”¨ç¬¬ä¸€é¡µä½œä¸ºå°é¢
        if not book.book_cover and triheart_page_models:
          book.book_cover = triheart_page_models[0].page_image_path
        book.process_status = PROC_SUCCESS
        # æœ€ç»ˆæ›´æ–°å¹¶æäº¤äº‹åŠ¡ï¼(commit=True)
        await self.update(user_id, book, commit=True)
        await task_manager.update_progress(task_id, 100, "ä¹¦ç±è§£æåŠä¸Šä¼ å…¨éƒ¨å®Œæˆ")
        self.logger.info(f"Task Success: Book {book.book_title} parsed.")

      else:
        # ä¸šåŠ¡å¤±è´¥é€»è¾‘
        self.logger.error(f"Task Failed Logic: {msg}")
        book.process_status = PROC_FAILED
        book.remark = f"è§£æå¤±è´¥: {msg}"
        await self.update(user_id, book, commit=True)

    except Exception as e:
      self.logger.error(f"Task Exception: {e}", exc_info=True)
      # å‘ç”Ÿå¼‚å¸¸å›æ»šäº‹åŠ¡
      await self.db.rollback()

      # å°è¯•è®°å½•é”™è¯¯çŠ¶æ€ï¼ˆéœ€è¦æ–°çš„äº‹åŠ¡ï¼‰
      try:
        # æ³¨æ„ï¼šrollbackå session ä¾ç„¶å¯ç”¨ï¼Œä½†éœ€è¦å°å¿ƒä½¿ç”¨
        book.process_status = PROC_FAILED
        book.remark = f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}"
        await self.update(user_id, book, commit=True)
      except:
        pass

  async def ai_extraction_logic(self, user_id: str, book_id: str, from_page: int, to_page: int):
    """
    ä»…æ‰§è¡Œ AI æ–‡æœ¬åˆ†æï¼Œç”Ÿæˆ Term è®°å½•ï¼Œä¸æ“ä½œ PDF
    """
    self.logger.info(f"ğŸ¤– [AI Task] å¼€å§‹æå–: Book={book_id}, Range={from_page}-{to_page}")

    term_service = TriHeartTermService(self.db)
    page_service = TriHeartPageService(self.db)

    # 1. æŸ¥ä¹¦
    book = await self.get(user_id, book_id)
    if not book:
      self.logger.error("ä¹¦ç±ä¸å­˜åœ¨")
      return

    if not app_settings.AI_ENABLE:
      self.logger.error("AI åŠŸèƒ½æœªå¼€å¯")
      return

    # 2. æŸ¥æ–‡æœ¬ (ä»æ•°æ®åº“ Page è¡¨æŸ¥ï¼Œä¸éœ€è¦ PDF æ–‡ä»¶)
    page_query = TriHeartPageQuery(book_id=book_id, begin_page_no=from_page, end_page_no=to_page)
    pages = await page_service.query_all(user_id, page_query)

    full_text = "\n".join([p.page_content for p in pages if p.page_content])
    if not full_text or len(full_text) < 50:
      self.logger.warning("æŒ‡å®šèŒƒå›´å†…å®¹å¤ªå°‘ï¼Œè·³è¿‡ AI æå–")
      return

    # 3. è°ƒç”¨ AI
    ai_helper = AiHelper(api_key=app_settings.AI_API_KEY, base_url=app_settings.AI_BASE_URL, model=app_settings.AI_MODEL_NAME, max_tokens=app_settings.AI_MAX_TOKENS)
    try:
      ai_terms_list = await ai_helper.extract_terms_from_text(full_text)
    except Exception as e:
      self.logger.error(f"AI API è°ƒç”¨å¤±è´¥: {e}")
      return

    if not ai_terms_list:
      self.logger.info("AI æœªè¿”å›ä»»ä½•æœ¯è¯­")
      return

    # 4. å…¥åº“å»é‡
    term_query = TriHeartTermQuery(book_id=book_id)
    existing_terms = await term_service.query_all(user_id, term_query)
    existing_term_keys = {t.term_key for t in existing_terms}

    new_terms = []
    for item in ai_terms_list:
      key = item['term']
      desc = item['desc']
      if key not in existing_term_keys:
        new_terms.append(TriHeartTermModel(
            book_id=book_id,
            source_type="2",  # AIæŠ½å–
            term_key=key,
            term_explanation=desc
        ))
        existing_term_keys.add(key)  # é˜²æ­¢æœ¬æ¬¡æ‰¹æ¬¡å†…éƒ¨é‡å¤

    if new_terms:
      await term_service.create_batch(user_id, new_terms, commit=True)
      self.logger.info(f"âœ… [AI Task] æ–°å¢æœ¯è¯­ {len(new_terms)} æ¡")
    else:
      self.logger.info("âœ… [AI Task] æ²¡æœ‰å‘ç°æ–°æœ¯è¯­")

  async def scan_coordinates_logic(self, user_id: str, book_id: str):
    """
    å…¨ä¹¦æ‰«æï¼š
    1. ä¸‹è½½ PDF (å¦‚æœä¸å­˜åœ¨)
    2. è·å–è¯¥ä¹¦æ‰€æœ‰ Term
    3. ä½¿ç”¨ PyMuPDF æ‰«ææ‰€æœ‰é¡µé¢çš„åæ ‡
    4. è¦†ç›–å†™å…¥ PageTerm å…³è”è¡¨
    """
    self.logger.info(f"ğŸ” [Scan Task] å¼€å§‹å…¨ä¹¦åæ ‡æ‰«æ: Book={book_id}")

    term_service = TriHeartTermService(self.db)
    page_term_service = TriHeartPageTermService(self.db)

    # 1. å‡†å¤‡ä¹¦ç±å’Œæ–‡ä»¶
    book = await self.get(user_id, book_id)
    if not book or not book.book_pdf_path:
      self.logger.error("ä¹¦ç±æ— æ•ˆæˆ–æ—  PDF è·¯å¾„")
      return

    var_prefix = "var/"
    # å…¼å®¹ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
    relative_pdf_path = book.book_pdf_path.lstrip("/")
    local_pdf_path = f"{var_prefix}{relative_pdf_path}"

    # ã€æ ¸å¿ƒä¿®å¤ã€‘ç¡®ä¿ PDF å­˜åœ¨
    if not os.path.exists(local_pdf_path):
      self.logger.info(f"æœ¬åœ°ç¼ºå°‘ PDFï¼Œæ­£åœ¨ä» OSS ä¸‹è½½: {book.book_pdf_path}")
      try:
        os.makedirs(os.path.dirname(local_pdf_path), exist_ok=True)
        pdf_signed_url = await self.get_oss_download_sign_url(user_id, book.book_pdf_path)

        async with httpx.AsyncClient() as client:
          resp = await client.get(pdf_signed_url)
          if resp.status_code == 200:
            with open(local_pdf_path, "wb") as f:
              f.write(resp.content)
            self.logger.info("PDF ä¸‹è½½æˆåŠŸ")
          else:
            self.logger.error(f"PDF ä¸‹è½½å¤±è´¥ HTTP {resp.status_code}")
            return
      except Exception as e:
        self.logger.error(f"PDF ä¸‹è½½å¼‚å¸¸: {e}")
        return

    # 2. è·å–æ‰€æœ‰å…³é”®è¯
    term_query = TriHeartTermQuery(book_id=book_id)
    all_terms = await term_service.query_all(user_id, term_query)
    if not all_terms:
      self.logger.warning("è¯¥ä¹¦æ²¡æœ‰ä»»ä½•æœ¯è¯­ï¼Œæ— éœ€æ‰«æ")
      return

    term_map = {t.term_key: t.model_id for t in all_terms}
    target_keywords = list(term_map.keys())

    self.logger.info(f"å¾…æ‰«æå…³é”®è¯æ•°: {len(target_keywords)}")

    # 3. æ‰§è¡Œæ‰«æ (CPU å¯†é›†å‹ï¼Œæ”¾å…¥çº¿ç¨‹æ± )
    # æ‰«æå…¨ä¹¦ï¼š1 åˆ° book_page_count
    total_pages = book.book_page_count or 1000

    scan_result = await run_in_threadpool(
        PdfHelper.scan_terms_in_range,
        pdf_path=local_pdf_path,
        from_page=1,
        to_page=total_pages,
        keywords=target_keywords
    )

    if not scan_result:
      self.logger.info("æœªåŒ¹é…åˆ°ä»»ä½•åæ ‡")
      return

    # 4. å…¥åº“ (å…ˆåˆ ååŠ )
    # æ¸…ç†è¯¥ä¹¦æ‰€æœ‰çš„æ—§åæ ‡è®°å½•
    await page_term_service.delete_query(user_id, TriHeartPageTermQuery(book_id=book_id), commit=False)

    new_page_terms = []
    for page_no, matches in scan_result.items():
      for match in matches:
        term_key = match['term']
        rects = match['rects']
        term_id = term_map.get(term_key)

        if term_id:
          new_page_terms.append(TriHeartPageTermModel(
              book_id=book_id,
              page_no=page_no,
              term_id=term_id,
              term_key=term_key,
              rects_json=rects
          ))

    if new_page_terms:
      # æ‰¹é‡å†™å…¥
      # æ³¨æ„ï¼šå¦‚æœæ•°æ®é‡ç‰¹åˆ«å¤§ï¼ˆå¦‚å‡ ä¸‡æ¡ï¼‰ï¼Œå»ºè®®åˆ†æ‰¹å†™å…¥ï¼Œè¿™é‡Œå‡è®¾ä¸€èˆ¬å°±åœ¨å‡ åƒæ¡ä»¥å†…
      await page_term_service.create_batch(user_id, new_page_terms, commit=True)
      self.logger.info(f"âœ… [Scan Task] æ‰«æå®Œæˆï¼Œæ›´æ–°äº† {len(new_page_terms)} æ¡åæ ‡å…³è”")
    else:
      await page_term_service.get_crud().commit()  # æäº¤åˆ é™¤æ“ä½œ
      self.logger.info("âœ… [Scan Task] æ‰«æå®Œæˆï¼Œä½†æ²¡æœ‰åŒ¹é…é¡¹ (å·²æ¸…ç†æ—§æ•°æ®)")


class PageRectsMixinService(Generic[M]):
  """
  Mixin Service for handling coordinate transformation for models
  that contain page_no, book_id, and a rects_json-like field.
  """
  db: Any  # Will be set by the main service
  logger: Any

  def __init__(self, db):
    self.db = db
    self.logger = database.logger  # Use the logger from the database module

  async def _get_page_crop_data_map(self, user_id: str | None, book_id: str, page_numbers: list[int]) -> dict[tuple[str, int], str | None]:
    """
    Helper to batch query `crop_box_data` for a set of (book_id, page_no) tuples.
    """
    if not book_id or not page_numbers:
      return {}

    page_crop_data_map: Dict[Tuple[str, int], str | None] = {}

    pages = await TriHeartPageService(self.db).query_all(user_id, TriHeartPageQuery(book_id=book_id, page_numbers=page_numbers))
    for page in pages:
      # Access by index since row is a Row type
      page_crop_data_map[(page.book_id, page.page_no)] = page.crop_box_data

    return page_crop_data_map

  def _transform_rects_for_page_mode(self, original_rects: list[list[float]], crop_box_data: str | None) -> list[list[float]]:
    """
    Transforms coordinates from original image percentage to cropped image percentage.
    """
    if not original_rects:
      return []

    view_window = [0.0, 0.0, 1.0, 1.0]  # Default to full image
    if crop_box_data:
      try:
        parsed_view_window = json.loads(crop_box_data)
        if isinstance(parsed_view_window, list) and len(parsed_view_window) == 4 and all(isinstance(val, (int, float)) for val in parsed_view_window):
          view_window = parsed_view_window
        else:
          self.logger.warning(f"Invalid crop_box_data format: {crop_box_data}. Using default.")
      except json.JSONDecodeError:
        self.logger.warning(f"Failed to parse crop_box_data JSON: {crop_box_data}. Using default.")
      except Exception as e:
        self.logger.error(f"Error processing crop_box_data: {e}. Using default.")

    vx, vy, vw, vh = view_window

    # If it's a full image (no crop), no transformation needed
    if vx == 0.0 and vy == 0.0 and vw == 1.0 and vh == 1.0:
      return original_rects

    new_rects = []
    for r in original_rects:
      # r: [x, y, w, h] (original image percentage)
      ox, oy, ow, oh = r

      # Coordinate transformation algorithm
      # 1. Subtract viewport offset (becomes distance relative to cropped image's top-left)
      # 2. Divide by viewport width/height (becomes percentage relative to cropped image size)
      nx = (ox - vx) / vw
      ny = (oy - vy) / vh
      nw = ow / vw
      nh = oh / vh

      # Simple boundary check: if it's completely outside the cropped area, discard
      if nx + nw < 0 or nx > 1 or ny + nh < 0 or ny > 1:
        continue

      new_rects.append([round(nx, 4), round(ny, 4), round(nw, 4), round(nh, 4)])  # Retain 4 decimal places

    return new_rects

  def _transform_rects_to_original_mode(self, current_rects: list[list[float]], crop_box_data: str | None) -> list[list[float]]:
    """
    Transforms coordinates from cropped image percentage back to original image percentage.
    This is used when saving highlight_rects captured in 'crop' mode to 'original' mode in DB.
    """
    if not current_rects:
      return []

    view_window = [0.0, 0.0, 1.0, 1.0]  # Default to full image
    if crop_box_data:
      try:
        parsed_view_window = json.loads(crop_box_data)
        if isinstance(parsed_view_window, list) and len(parsed_view_window) == 4 and all(isinstance(val, (int, float)) for val in parsed_view_window):
          view_window = parsed_view_window
        else:
          self.logger.warning(f"Invalid crop_box_data format: {crop_box_data}. Using default.")
      except json.JSONDecodeError:
        self.logger.warning(f"Failed to parse crop_box_data JSON: {crop_box_data}. Using default.")
      except Exception as e:
        self.logger.error(f"Error processing crop_box_data: {e}. Using default.")

    vx, vy, vw, vh = view_window

    if vx == 0.0 and vy == 0.0 and vw == 1.0 and vh == 1.0:
      return current_rects

    new_rects = []
    for r in current_rects:
      cx, cy, cw, ch = r

      ox = vx + (cx * vw)
      oy = vy + (cy * vh)
      ow = cw * vw
      oh = ch * vh

      new_rects.append([round(ox, 4), round(oy, 4), round(ow, 4), round(oh, 4)])  # Retain 4 decimal places

    return new_rects

  async def rects_transform_batch(self, image_mode: str, book_id: str, page_numbers: list[int], models: list[M], rects_field_name: str) -> None:
    """
    Performs coordinate transformation on a batch of models if image_mode is 'crop'.
    It expects models to have `book_id`, `page_no`, and the specified `rects_field_name` attribute.
    """
    if not models or not book_id or not page_numbers:
      return

    if image_mode == 'crop':
      page_crop_data_map = await self._get_page_crop_data_map(None, book_id, page_numbers)

      for model in models:
        if hasattr(model, 'book_id') and hasattr(model, 'page_no') and model.book_id and hasattr(model, rects_field_name):
          original_rects = getattr(model, rects_field_name)
          crop_box_data = page_crop_data_map.get((model.book_id, model.page_no))
          transformed_rects = self._transform_rects_for_page_mode(original_rects, crop_box_data)
          setattr(model, rects_field_name, transformed_rects)


class TriHeartBookUserService(StringPKeyWithDictionaryService[TriHeartBookUserModel, TriHeartBookUserCrud, TriHeartBookUserQuery], PaymentServiceMixin):

  async def unique_constraints(self, models: Sequence[TriHeartBookUserModel]) -> list[UniqueConstraint] | None:
    constraints = []
    if c := UniqueConstraint.from_models(models, ['book_id', 'user_id']):
      constraints.append(c)
    return constraints

  async def pre_create(self, user_id: str, model: TriHeartBookUserModel) -> None:
    await super().pre_create(user_id, model)
    if not model.user_id:
      model.user_id = user_id

  async def pre_select(self, user_id: str, query: TriHeartBookUserQuery) -> None:
    if not query.user_id:
      query.user_id = user_id
    await super().pre_select(user_id, query)

  async def post_select_batch(self, user_id: str, models: list[TriHeartBookUserModel], query: TriHeartBookUserQuery | None = None) -> None:
    await super().post_select_batch(user_id, models, query)
    # 0. åˆ¤ç©º
    if not models:
      return

    # 1. æå–å»é‡åçš„ book_id åˆ—è¡¨
    book_ids = list({model.book_id for model in models if model.book_id})
    if not book_ids:
      return

    # 2. è°ƒç”¨ BookService æŸ¥è¯¢ä¹¦ç±è¯¦æƒ…
    # æ³¨æ„ï¼šå®ä¾‹åŒ– Service æ—¶ä¼ å…¥å½“å‰ self.dbï¼Œå¤ç”¨æ•°æ®åº“ä¼šè¯
    book_service = TriHeartBookService(self.db)

    # ä½¿ç”¨ CRUD åº•å±‚çš„ get_by_ids æ‰¹é‡æŸ¥è¯¢ï¼Œæ•ˆç‡æœ€é«˜
    # (è¿™é‡Œä¸å»ºè®®è°ƒ Service å±‚çš„æŸ¥è¯¢æ–¹æ³•ï¼Œå› ä¸ºå¯èƒ½ä¼šå—åˆ° owner_id æƒé™è¿‡æ»¤çš„å½±å“ï¼Œå¯¼è‡´ç”¨æˆ·æŸ¥ä¸åˆ°ç®¡ç†å‘˜å‘å¸ƒçš„ä¹¦)
    books = await book_service.get_by_ids(user_id, book_ids)

    # 3. å°†ä¹¦ç±åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ {book_id: book_obj}ï¼Œæ–¹ä¾¿ O(1) æŸ¥æ‰¾
    book_map = {b.model_id: b for b in books}

    # 4. éå†åŸåˆ—è¡¨ï¼Œå›å¡«æ•°æ®
    for model in models:
      if model.book_id in book_map:
        # å¯¹åº” models.py ä¸­ TriHeartBookUserModel å®šä¹‰çš„ book_models å­—æ®µ (Listç±»å‹)
        model.book_model = book_map[model.book_id]

  def resolve_pay_order(self, order: PayOrderModel) -> TriHeartBookUserModel:
    return TriHeartBookUserModel(book_id=order.business_id, user_id=order.user_id, purchase_status="1")

  def resolve_update_fields(self, order: PayOrderModel, model: TriHeartBookUserModel) -> set[str] | None:
    return {"purchase_status"}


class TriHeartBookNoteService(StringPKeyWithDictionaryService[TriHeartBookNoteModel, TriHeartBookNoteCrud, TriHeartBookNoteQuery], PageRectsMixinService[TriHeartBookNoteModel]):
  def __init__(self, db: Any):
    super().__init__(db=db)

  async def pre_create(self, user_id: str, model: TriHeartBookNoteModel) -> None:
    await super().pre_create(user_id, model)
    if not model.user_id:
      model.user_id = user_id

    if model.crop_mode == True and model.book_id and model.page_no and model.highlight_rects:
      page_service = TriHeartPageService(self.db)
      page = await page_service.query_one(user_id, TriHeartPageQuery(book_id=model.book_id, page_no=model.page_no))

      if page and page.crop_box_data:
        cropped_rects_from_frontend = model.highlight_rects
        transformed_rects_to_original = self._transform_rects_to_original_mode(cropped_rects_from_frontend, page.crop_box_data)
        model.highlight_rects = transformed_rects_to_original
        self.logger.info(f"BookNote pre_create: Transformed rects from crop to original mode for book={model.book_id}, page={model.page_no}")
      else:
        self.logger.warning(f"BookNote pre_create: Could not find crop_box_data for book={model.book_id}, page={model.page_no} or page not found. Storing rects as is.")

  async def pre_update(self, user_id: str, old_model: TriHeartBookNoteModel, new_model: TriHeartBookNoteModel, update_fields: set[str]) -> None:
    await super().pre_update(user_id, old_model, new_model, update_fields)
    update_fields.remove("highlight_rects")

  async def post_select_batch(self, user_id: str | None, models: list[TriHeartBookNoteModel], query: TriHeartBookNoteQuery | None = None) -> None:
    await super().post_select_batch(user_id, models, query)

    if not models or query is None or query.image_mode is None:
      return

    page_nos = list(set(model.page_no for model in models if model.page_no))
    await self.rects_transform_batch(query.image_mode, query.book_id, page_nos, models, "highlight_rects")


class TriHeartTermService(StringPKeyWithDictionaryService[TriHeartTermModel, TriHeartTermCrud, TriHeartTermQuery]):
  pass


class TriHeartPageTermService(StringPKeyWithDictionaryService[TriHeartPageTermModel, TriHeartPageTermCrud, TriHeartPageTermQuery], PageRectsMixinService[TriHeartPageTermModel]):

  def __init__(self, db: Any):
    super().__init__(db=db)

  async def post_select_batch(self, user_id: str, models: list[TriHeartPageTermModel], query: TriHeartPageTermQuery | None = None) -> None:
    # 1. æ‰§è¡Œçˆ¶ç±»é€»è¾‘ï¼ˆå¤„ç†å­—å…¸è½¬æ¢ç­‰ï¼‰
    await super().post_select_batch(user_id, models, query)

    if not models:
      return

    term_ids = list(set(model.term_id for model in models if model.term_id))
    if term_ids:
      term_map: dict[str, TriHeartTermModel] = {}
      term_service = TriHeartTermService(self.db)
      terms = await term_service.get_by_ids(user_id, term_ids)
      if terms:
        term_map = {t.model_id: t for t in terms}
        for model in models:
          term = term_map.get(model.term_id)
          if not term:
            continue
          model.term_key = term.term_key
          model.term_explanation = term.term_explanation

    page_nos = list(set(model.page_no for model in models if model.page_no))
    await self.rects_transform_batch(query.image_mode, query.book_id, page_nos, models, "rects_json")


class TriHeartPageAttachmentService(StringPKeyWithDictionaryService[TriHeartPageAttachmentModel, TriHeartPageAttachmentCrud, TriHeartPageAttachmentQuery]):
  pass


PaymentDispatcher.register_service("book_purchase", TriHeartBookUserService)
